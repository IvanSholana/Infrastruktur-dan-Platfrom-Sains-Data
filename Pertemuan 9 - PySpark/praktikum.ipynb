{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice ID: string (nullable = true)\n",
      " |-- Branch: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Customer type: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Product line: string (nullable = true)\n",
      " |-- Unit price: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Tax 5%: double (nullable = true)\n",
      " |-- Total: double (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Payment: string (nullable = true)\n",
      " |-- cogs: double (nullable = true)\n",
      " |-- gross margin percentage: double (nullable = true)\n",
      " |-- gross income: double (nullable = true)\n",
      " |-- Rating: double (nullable = true)\n",
      "\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+---------+-------------------+-----------+------+-----------------------+------------+------+\n",
      "| Invoice ID|Branch|     City|Customer type|Gender|        Product line|Unit price|Quantity| Tax 5%|   Total|     Date|               Time|    Payment|  cogs|gross margin percentage|gross income|Rating|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+---------+-------------------+-----------+------+-----------------------+------------+------+\n",
      "|750-67-8428|     A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|548.9715| 1/5/2019|2024-12-09 13:08:00|    Ewallet|522.83|            4.761904762|     26.1415|   9.1|\n",
      "|226-31-3081|     C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|   80.22| 3/8/2019|2024-12-09 10:29:00|       Cash|  76.4|            4.761904762|        3.82|   9.6|\n",
      "|631-41-3108|     A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|340.5255| 3/3/2019|2024-12-09 13:23:00|Credit card|324.31|            4.761904762|     16.2155|   7.4|\n",
      "|123-19-1176|     A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 489.048|1/27/2019|2024-12-09 20:33:00|    Ewallet|465.76|            4.761904762|      23.288|   8.4|\n",
      "|373-73-7910|     A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|634.3785| 2/8/2019|2024-12-09 10:37:00|    Ewallet|604.17|            4.761904762|     30.2085|   5.3|\n",
      "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+---------+-------------------+-----------+------+-----------------------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.appName(\"Supermarket Sales Analysis with Training\").getOrCreate()\n",
    "\n",
    "# Membaca dataset\n",
    "file_path = \"supermarket_sales - Sheet1.csv\"  # Pastikan file berada di lokasi yang sama\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Menampilkan skema data\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+-------+------+\n",
      "|Unit price|Quantity|   Total| Tax 5%|Rating|\n",
      "+----------+--------+--------+-------+------+\n",
      "|     74.69|       7|548.9715|26.1415|   9.1|\n",
      "|     15.28|       5|   80.22|   3.82|   9.6|\n",
      "|     46.33|       7|340.5255|16.2155|   7.4|\n",
      "|     58.22|       8| 489.048| 23.288|   8.4|\n",
      "|     86.31|       7|634.3785|30.2085|   5.3|\n",
      "+----------+--------+--------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Memilih kolom yang relevan\n",
    "df_selected = df.select(\"Unit price\", \"Quantity\", \"Total\", \"Tax 5%\", \"Rating\")\n",
    "\n",
    "# Menghapus baris dengan nilai null\n",
    "df_cleaned = df_selected.dropna()\n",
    "\n",
    "# Menampilkan data setelah dibersihkan\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|Rating|\n",
      "+--------------------+------+\n",
      "|[74.69,7.0,548.97...|   9.1|\n",
      "|[15.28,5.0,80.22,...|   9.6|\n",
      "|[46.33,7.0,340.52...|   7.4|\n",
      "|[58.22,8.0,489.04...|   8.4|\n",
      "|[86.31,7.0,634.37...|   5.3|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Menggabungkan kolom fitur\n",
    "feature_columns = [\"Unit price\", \"Quantity\", \"Total\", \"Tax 5%\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Menambahkan kolom fitur ke DataFrame\n",
    "df_features = assembler.transform(df_cleaned)\n",
    "\n",
    "# Menampilkan data dengan kolom fitur\n",
    "df_features.select(\"features\", \"Rating\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data latih: 838\n",
      "Jumlah data uji: 162\n"
     ]
    }
   ],
   "source": [
    "# Membagi data menjadi training dan testing\n",
    "train_data, test_data = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Jumlah data latih: {train_data.count()}\")\n",
    "print(f\"Jumlah data uji: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koefisien: [0.007178226344261361,0.06323490118032725,-0.0006097756155870004,-0.0128052879273355]\n",
      "Intercept: 6.6194781648684575\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Membuat model regresi linier\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Melatih model pada data latih\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Menampilkan koefisien dan intercept\n",
    "print(f\"Koefisien: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|            features|Rating|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|[10.16,5.0,53.34,...|   4.1|6.9435325877569465|\n",
      "|[10.56,8.0,88.704...|   7.6| 7.092980372096381|\n",
      "|[10.69,5.0,56.122...|   7.6| 6.943943646418662|\n",
      "|[11.43,6.0,72.009...|   7.7| 6.993116034459691|\n",
      "|[12.05,5.0,63.262...|   5.5| 6.944998438456272|\n",
      "+--------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE): 1.6902525632142653\n"
     ]
    }
   ],
   "source": [
    "# Membuat prediksi pada data uji\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Menampilkan hasil prediksi\n",
    "predictions.select(\"features\", \"Rating\", \"prediction\").show(5)\n",
    "\n",
    "# Menghitung metrik evaluasi\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"Rating\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
